{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "8cb5116a0621410f9d9533c2f27d24d5": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_ee7f9bea94f0406ba83e3a84e91769f6",
              "IPY_MODEL_1945496256644a2093ee26c5e3106459",
              "IPY_MODEL_1b08cc9794bb4218962db32f2d7ecb4a"
            ],
            "layout": "IPY_MODEL_ce489bd591a64d0e81d0b94245c9e9d0"
          }
        },
        "ee7f9bea94f0406ba83e3a84e91769f6": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_747d830fed9a47ba8788c07ecb724bcb",
            "placeholder": "​",
            "style": "IPY_MODEL_707a4a97e2ff4416be08adaefeea471a",
            "value": "Loading checkpoint shards: 100%"
          }
        },
        "1945496256644a2093ee26c5e3106459": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_c9a4e40a64474bffb3262f9d5f9a3d3a",
            "max": 2,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_581b61af875744d78ba16764542e4393",
            "value": 2
          }
        },
        "1b08cc9794bb4218962db32f2d7ecb4a": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_de479a1a17bf47cdbfe287e59d06c7fc",
            "placeholder": "​",
            "style": "IPY_MODEL_658728129eb1459cb1828317b6b0d6a9",
            "value": " 2/2 [01:13&lt;00:00, 33.45s/it]"
          }
        },
        "ce489bd591a64d0e81d0b94245c9e9d0": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "747d830fed9a47ba8788c07ecb724bcb": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "707a4a97e2ff4416be08adaefeea471a": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "c9a4e40a64474bffb3262f9d5f9a3d3a": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "581b61af875744d78ba16764542e4393": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "de479a1a17bf47cdbfe287e59d06c7fc": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "658728129eb1459cb1828317b6b0d6a9": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "4c77202c461e48f4b2bd62d500ec4cad": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_e23253352a134a2cbc47e076eac02bf8",
              "IPY_MODEL_d7eea3bcd2a84fc2a8f7e5d9a36c219a",
              "IPY_MODEL_3ba2099a6b5a4b92b8d3e7e45a859702"
            ],
            "layout": "IPY_MODEL_469577344502417daeb291a9cea25851"
          }
        },
        "e23253352a134a2cbc47e076eac02bf8": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_aa8c885dbd004ac299db0034dcb56793",
            "placeholder": "​",
            "style": "IPY_MODEL_b2d941aa8b264bf2a0051f7387365e20",
            "value": "Loading checkpoint shards: 100%"
          }
        },
        "d7eea3bcd2a84fc2a8f7e5d9a36c219a": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_0dcb612a09864aabba31732fab6a0b5d",
            "max": 2,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_b4d82562c0cd4e8bb9148c67155172a0",
            "value": 2
          }
        },
        "3ba2099a6b5a4b92b8d3e7e45a859702": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_e50b508edc7842f7b2a670f96306c1cc",
            "placeholder": "​",
            "style": "IPY_MODEL_7dfe97d0b90a4381b36a21dfdc91291d",
            "value": " 2/2 [00:34&lt;00:00, 14.32s/it]"
          }
        },
        "469577344502417daeb291a9cea25851": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "aa8c885dbd004ac299db0034dcb56793": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "b2d941aa8b264bf2a0051f7387365e20": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "0dcb612a09864aabba31732fab6a0b5d": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "b4d82562c0cd4e8bb9148c67155172a0": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "e50b508edc7842f7b2a670f96306c1cc": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "7dfe97d0b90a4381b36a21dfdc91291d": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "f7807694a26642aaa0823cbe756b35ee": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_52f96d8dd15b48deabd7c220fe7a56ce",
              "IPY_MODEL_c2b2194ce98a4ec6b4f147521c5c0d37",
              "IPY_MODEL_973c643070c54575a3cbb0bddd194b42"
            ],
            "layout": "IPY_MODEL_23740995c9594ec794c91ea26bc4b282"
          }
        },
        "52f96d8dd15b48deabd7c220fe7a56ce": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_31e5af81d57741d38ee568449fbcd4ee",
            "placeholder": "​",
            "style": "IPY_MODEL_e1542165bc7a43b8ab2df35402850244",
            "value": "Loading checkpoint shards: 100%"
          }
        },
        "c2b2194ce98a4ec6b4f147521c5c0d37": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_fd7d60962d0f46b5999bf84bccc27712",
            "max": 2,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_4e7a61aa2cbc4d24861cf9689b538f2c",
            "value": 2
          }
        },
        "973c643070c54575a3cbb0bddd194b42": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_a8f253f1570448009b1b90b70486a427",
            "placeholder": "​",
            "style": "IPY_MODEL_917a9a1c3de54a7ab2ba7659be6e2dc4",
            "value": " 2/2 [00:00&lt;00:00, 17.19it/s]"
          }
        },
        "23740995c9594ec794c91ea26bc4b282": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "31e5af81d57741d38ee568449fbcd4ee": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "e1542165bc7a43b8ab2df35402850244": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "fd7d60962d0f46b5999bf84bccc27712": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "4e7a61aa2cbc4d24861cf9689b538f2c": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "a8f253f1570448009b1b90b70486a427": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "917a9a1c3de54a7ab2ba7659be6e2dc4": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "446fd3c6f9ae4983b28b66989d79e242": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_86c2f6882648449ab0f88af038c86c9a",
              "IPY_MODEL_03f2a98df14e4e679a159dcf09f8bb42",
              "IPY_MODEL_a9fedf156bc74dfab74ac7095f24d9e5"
            ],
            "layout": "IPY_MODEL_4929b1fe96044a778414308792f3768d"
          }
        },
        "86c2f6882648449ab0f88af038c86c9a": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_ee386f0a6fc84349b7006d2c348ca629",
            "placeholder": "​",
            "style": "IPY_MODEL_8ff4929ced5c49608a956e5de3055f92",
            "value": "Loading checkpoint shards: 100%"
          }
        },
        "03f2a98df14e4e679a159dcf09f8bb42": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_578363f9bb2f4fdb8f61c0459b26efe6",
            "max": 2,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_bab683bfb2b845d488a95b15008bfdef",
            "value": 2
          }
        },
        "a9fedf156bc74dfab74ac7095f24d9e5": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_99bb320e1791452abab596580854183a",
            "placeholder": "​",
            "style": "IPY_MODEL_b921b00859cf4a149b7834bd81bfc9f9",
            "value": " 2/2 [01:20&lt;00:00, 36.83s/it]"
          }
        },
        "4929b1fe96044a778414308792f3768d": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "ee386f0a6fc84349b7006d2c348ca629": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "8ff4929ced5c49608a956e5de3055f92": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "578363f9bb2f4fdb8f61c0459b26efe6": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "bab683bfb2b845d488a95b15008bfdef": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "99bb320e1791452abab596580854183a": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "b921b00859cf4a149b7834bd81bfc9f9": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        }
      }
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Fine-tuning Llama 2 (7B Chat) for KIIT QnA with LoRA\n",
        "\n",
        "This notebook demonstrates how to fine-tune the `meta-llama/Llama-2-7b-chat-hf` model on a custom KIIT Question-Answering dataset using Parameter-Efficient Fine-Tuning (PEFT) with LoRA and the `trl` library.\n",
        "\n",
        "**Steps:**\n",
        "1. Install necessary libraries.\n",
        "2. Load the base Llama 2 model and tokenizer (quantized).\n",
        "3. Load the custom KIIT dataset (`kiit_data.jsonl`).\n",
        "4. Configure LoRA (`PeftModel`).\n",
        "5. Configure Training Arguments.\n",
        "6. Initialize and run the `SFTTrainer`.\n",
        "7. Save the LoRA adapters.\n",
        "8. Merge adapters with the base model and save the final model locally.\n",
        "9. Test inference with the fine-tuned model.\n",
        "10. Test Inference using Adapters Directly (No Merge)\n"
      ],
      "metadata": {
        "id": "W07AoXjMHjFC"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Step 1: Install Libraries\n"
      ],
      "metadata": {
        "id": "E2vFMTnrHxwY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -q accelerate peft bitsandbytes transformers trl datasets torch\n"
      ],
      "metadata": {
        "id": "Qv_sZQHzPkvO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Step 2: Load Model, Tokenizer, and Configure Quantization\n"
      ],
      "metadata": {
        "id": "wQuVW2HMIHQ6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from transformers import (\n",
        "    AutoModelForCausalLM,\n",
        "    AutoTokenizer,\n",
        "    BitsAndBytesConfig,\n",
        "    HfArgumentParser,\n",
        "    TrainingArguments,\n",
        "    pipeline,\n",
        "    logging,\n",
        ")\n",
        "from peft import LoraConfig, PeftModel\n",
        "from trl import SFTTrainer\n",
        "from datasets import load_dataset\n",
        "import os\n",
        "\n",
        "# Suppress warnings\n",
        "logging.set_verbosity(logging.CRITICAL)\n"
      ],
      "metadata": {
        "id": "butnZv-TIIt2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# --- Configuration ---\n",
        "\n",
        "# Model from Hugging Face hub (Make sure you have access requested and granted)\n",
        "base_model_name = \"meta-llama/Llama-2-7b-chat-hf\"\n",
        "\n",
        "# Custom KIIT dataset path\n",
        "dataset_path = \"kiit_data.jsonl\" # Ensure this file is accessible\n",
        "\n",
        "# Output directory for LoRA adapters\n",
        "output_adapter_dir = \"kiit-llama2-7b-chat-lora-adapters\"\n",
        "\n",
        "# Output directory for the final merged model (for local use/Streamlit)\n",
        "final_merged_model_dir = \"kiit-llama2-7b-chat-final\"\n",
        "\n",
        "# --- Quantization Config ---\n",
        "# Use 4-bit quantization to reduce memory usage\n",
        "compute_dtype = getattr(torch, \"float16\") # Or bfloat16 if supported (Ampere+)\n",
        "\n",
        "quant_config = BitsAndBytesConfig(\n",
        "    load_in_4bit=True,\n",
        "    bnb_4bit_quant_type=\"nf4\",\n",
        "    bnb_4bit_compute_dtype=compute_dtype,\n",
        "    bnb_4bit_use_double_quant=False, # Optional\n",
        ")\n",
        "\n",
        "# --- Load Base Model ---\n",
        "print(f\"Loading base model: {base_model_name}...\")\n",
        "# Replace 'YOUR_HF_TOKEN' with your actual Hugging Face token.\n",
        "# Make sure to request access to the model on Hugging Face's website and get your token.\n",
        "HF_TOKEN = \"hf_*************************\"  # Replace with your Hugging Face token\n",
        "\n",
        "model = AutoModelForCausalLM.from_pretrained(\n",
        "    base_model_name,\n",
        "    quantization_config=quant_config,\n",
        "    device_map={\"\": 0}, # Automatically load model layers onto GPU 0\n",
        "    token=HF_TOKEN # Pass the token here\n",
        ")\n",
        "# Configure model for training\n",
        "model.config.use_cache = False # Disable caching for gradient checkpointing\n",
        "model.config.pretraining_tp = 1 # Tensor parallelism setting (usually 1 for single GPU)\n",
        "print(\"Base model loaded.\")\n",
        "\n",
        "# --- Load Tokenizer ---\n",
        "print(\"Loading tokenizer...\")\n",
        "tokenizer = AutoTokenizer.from_pretrained(base_model_name, trust_remote_code=True, token=HF_TOKEN) # Pass the token here as well\n",
        "# Set padding token to EOS token for autoregressive models\n",
        "tokenizer.pad_token = tokenizer.eos_token\n",
        "tokenizer.padding_side = \"right\" # Important for fp16 training\n",
        "print(\"Tokenizer loaded.\")"
      ],
      "metadata": {
        "id": "IsGqO47dISF5",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 118,
          "referenced_widgets": [
            "8cb5116a0621410f9d9533c2f27d24d5",
            "ee7f9bea94f0406ba83e3a84e91769f6",
            "1945496256644a2093ee26c5e3106459",
            "1b08cc9794bb4218962db32f2d7ecb4a",
            "ce489bd591a64d0e81d0b94245c9e9d0",
            "747d830fed9a47ba8788c07ecb724bcb",
            "707a4a97e2ff4416be08adaefeea471a",
            "c9a4e40a64474bffb3262f9d5f9a3d3a",
            "581b61af875744d78ba16764542e4393",
            "de479a1a17bf47cdbfe287e59d06c7fc",
            "658728129eb1459cb1828317b6b0d6a9"
          ]
        },
        "outputId": "f13bf2c3-4ba0-46a3-825d-7c18282d57f5"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loading base model: meta-llama/Llama-2-7b-chat-hf...\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "8cb5116a0621410f9d9533c2f27d24d5"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Base model loaded.\n",
            "Loading tokenizer...\n",
            "Tokenizer loaded.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Step 3: Load KIIT Dataset\n",
        "\n",
        "Load the JSON Lines file created earlier.\n"
      ],
      "metadata": {
        "id": "wMj1eDzOX5qE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Load the dataset\n",
        "try:\n",
        "    dataset = load_dataset('json', data_files=dataset_path, split=\"train\")\n",
        "    print(f\"Dataset loaded successfully from {dataset_path}\")\n",
        "    print(f\"Dataset size: {len(dataset)}\")\n",
        "    print(\"\\nFirst example:\")\n",
        "    # Print the structure of the first example's text\n",
        "    print(dataset[0]['text'].replace('\\\\n', '\\n')) # Make newlines readable\n",
        "except Exception as e:\n",
        "    print(f\"Error loading dataset from '{dataset_path}': {e}\")\n",
        "    print(\"Please ensure 'kiit_data.jsonl' exists in the correct path and is formatted correctly.\")\n",
        "    # Stop execution if dataset fails to load\n",
        "    raise SystemExit(\"Dataset loading failed.\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8ai8zOalX8st",
        "outputId": "acf2a75b-7cca-4196-f966-5994e65bfd77"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Dataset loaded successfully from kiit_data.jsonl\n",
            "Dataset size: 1047\n",
            "\n",
            "First example:\n",
            "<s>[INST] <<SYS>>\n",
            "You are a helpful assistant knowledgeable about Kalinga Institute of Industrial Technology (KIIT). Provide concise and accurate information based on the user's question about KIIT.\n",
            "<</SYS>>\n",
            "\n",
            "What does KIIT stand for? [/INST] KIIT stands for Kalinga Institute of Industrial Technology. Established in 1992, it's a deemed university located in Bhubaneswar, Odisha, India. </s>\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Step 4: Configure LoRA (PEFT)\n",
        "\n",
        "Configure LoRA to adapt specific layers of the base model efficiently.\n"
      ],
      "metadata": {
        "id": "5x0_QamSYD8T"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# LoRA configuration\n",
        "peft_config = LoraConfig(\n",
        "    lora_alpha=16,          # Scaling factor for LoRA weights\n",
        "    lora_dropout=0.1,       # Dropout probability for LoRA layers\n",
        "    r=64,                   # Rank of the LoRA matrices (higher rank = more parameters)\n",
        "    bias=\"none\",            # Bias terms to train ('none', 'all', 'lora_only')\n",
        "    task_type=\"CAUSAL_LM\",  # Task type\n",
        "    # Target modules specific to Llama 2 architecture\n",
        "    target_modules=[\n",
        "        \"q_proj\",\n",
        "        \"k_proj\",\n",
        "        \"v_proj\",\n",
        "        \"o_proj\",\n",
        "        \"gate_proj\",\n",
        "        \"up_proj\",\n",
        "        \"down_proj\",\n",
        "        \"embed_tokens\"\n",
        "\n",
        "    ]\n",
        ")\n",
        "\n",
        "print(\"LoRA Config set up.\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OuVPSpNAYFLQ",
        "outputId": "2b447f9b-23aa-471e-9e08-ba38fbafa66e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "LoRA Config set up.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Step 5: Configure Training Arguments\n",
        "\n",
        "Set hyperparameters for the training process. Adjust `num_train_epochs`, `per_device_train_batch_size`, and `gradient_accumulation_steps` based on your dataset size and GPU memory.\n"
      ],
      "metadata": {
        "id": "azccMZmmY3Gv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "\n",
        "# --- Set PyTorch CUDA Allocation Configuration ---\n",
        "# Recommended by the OOM error message to potentially reduce fragmentation\n",
        "os.environ[\"PYTORCH_CUDA_ALLOC_CONF\"] = \"expandable_segments:True\"\n",
        "print(f\"Set PYTORCH_CUDA_ALLOC_CONF={os.environ.get('PYTORCH_CUDA_ALLOC_CONF')}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fcSzSFhshQ66",
        "outputId": "4890dfa0-d407-4991-8f14-82403bd0cabf"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Set PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from trl import SFTConfig # Import SFTConfig\n",
        "\n",
        "# --- Check GPU Capability for fp16/bf16 ---\n",
        "use_bf16 = False # Default to False\n",
        "if torch.cuda.is_available():\n",
        "    major, minor = torch.cuda.get_device_capability()\n",
        "    if major >= 8: # Ampere or newer supports bfloat16\n",
        "        use_bf16 = True\n",
        "        print(\"Ampere+ GPU detected, enabling bf16.\")\n",
        "    else:\n",
        "        print(\"Older GPU detected, using fp16.\")\n",
        "else:\n",
        "    print(\"CUDA not available, cannot use bf16 or fp16.\")\n",
        "\n",
        "# Use SFTConfig to hold all arguments\n",
        "training_args = SFTConfig(\n",
        "    # --- Training Arguments ---\n",
        "    output_dir=\"./kiit-results\",\n",
        "    num_train_epochs=1,\n",
        "    per_device_train_batch_size=1,          # Keep batch size at 1\n",
        "    gradient_accumulation_steps=8,          # Keep accumulation\n",
        "    gradient_checkpointing=True,\n",
        "    gradient_checkpointing_kwargs={\"use_reentrant\": False},\n",
        "    optim=\"paged_adamw_8bit\",\n",
        "    save_strategy=\"steps\",\n",
        "    save_steps=100,\n",
        "    logging_steps=25,\n",
        "    learning_rate=2e-4,\n",
        "    weight_decay=0.001,\n",
        "    fp16=not use_bf16,\n",
        "    bf16=use_bf16,\n",
        "    max_grad_norm=0.3,\n",
        "    max_steps=-1,\n",
        "    warmup_ratio=0.03,\n",
        "    group_by_length=True,\n",
        "    lr_scheduler_type=\"cosine\",\n",
        "    report_to=\"tensorboard\",\n",
        "\n",
        "\n",
        "    max_seq_length=512,\n",
        "    dataset_text_field=\"text\",\n",
        "    packing=False,\n",
        ")\n",
        "\n",
        "print(f\"SFTConfig set up: fp16={training_args.fp16}, bf16={training_args.bf16}, max_seq_length={training_args.max_seq_length}, LoRA r=64.\")\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6cn-XFfzY4oo",
        "outputId": "71a71241-0058-494b-b4ac-e8aab2dd3d24"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Older GPU detected, using fp16.\n",
            "SFTConfig set up: fp16=True, bf16=False, max_seq_length=512, LoRA r=64.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Step 6: Initialize and Run Trainer (`SFTTrainer`)\n",
        "\n",
        "Initialize the `SFTTrainer` from `trl`, which simplifies the process of supervised fine-tuning.\n"
      ],
      "metadata": {
        "id": "dw98TCOGZAep"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "trainer = SFTTrainer(\n",
        "    model=model,\n",
        "    train_dataset=dataset,\n",
        "    peft_config=peft_config,\n",
        "    processing_class=tokenizer,\n",
        "    args=training_args\n",
        "\n",
        ")\n",
        "\n",
        "# Start training\n",
        "print(\"Starting fine-tuning...\")\n",
        "trainer.train()\n",
        "print(\"Fine-tuning finished.\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2_EbN4DyZETA",
        "outputId": "17c6f4f0-a741-44b7-c8a8-3ce1b53fcb84"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Starting fine-tuning...\n",
            "{'loss': 1.9617, 'grad_norm': 58.88863754272461, 'learning_rate': 0.00019308737486442045, 'num_tokens': 23253.0, 'mean_token_accuracy': 0.6693099531531334, 'epoch': 0.19102196752626552}\n",
            "{'loss': 0.9357, 'grad_norm': 1.1141765117645264, 'learning_rate': 0.00015425462638657595, 'num_tokens': 46159.0, 'mean_token_accuracy': 0.7916224443912506, 'epoch': 0.38204393505253104}\n",
            "{'loss': 0.7847, 'grad_norm': 0.8637145757675171, 'learning_rate': 9.501541143393028e-05, 'num_tokens': 68880.0, 'mean_token_accuracy': 0.8164774137735367, 'epoch': 0.5730659025787965}\n",
            "{'loss': 0.7508, 'grad_norm': 0.8939021825790405, 'learning_rate': 3.7651019814126654e-05, 'num_tokens': 91162.0, 'mean_token_accuracy': 0.8221593025326729, 'epoch': 0.7640878701050621}\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/peft/utils/save_and_load.py:230: UserWarning: Setting `save_embedding_layers` to `True` as embedding layers found in `target_modules`.\n",
            "  warnings.warn(\"Setting `save_embedding_layers` to `True` as embedding layers found in `target_modules`.\")\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'loss': 0.6828, 'grad_norm': 0.8343603014945984, 'learning_rate': 3.7375753049987973e-06, 'num_tokens': 113686.0, 'mean_token_accuracy': 0.8278827980160713, 'epoch': 0.9551098376313276}\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/peft/utils/save_and_load.py:230: UserWarning: Setting `save_embedding_layers` to `True` as embedding layers found in `target_modules`.\n",
            "  warnings.warn(\"Setting `save_embedding_layers` to `True` as embedding layers found in `target_modules`.\")\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'train_runtime': 1645.3758, 'train_samples_per_second': 0.636, 'train_steps_per_second': 0.079, 'train_loss': 1.011131897339454, 'num_tokens': 117925.0, 'mean_token_accuracy': 0.8320745095610619, 'epoch': 0.9933142311365807}\n",
            "Fine-tuning finished.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Step 7: Save LoRA Adapters\n",
        "\n",
        "Save the trained adapter weights. These are small files representing the changes made to the base model.\n"
      ],
      "metadata": {
        "id": "A774kF4nZKN1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Save the LoRA adapter weights\n",
        "print(f\"Saving LoRA adapters to {output_adapter_dir}...\")\n",
        "trainer.model.save_pretrained(output_adapter_dir)\n",
        "print(\"Adapters saved.\")\n",
        "\n",
        "# Save tokenizer files as well (good practice)\n",
        "print(f\"Saving tokenizer to {output_adapter_dir}...\")\n",
        "tokenizer.save_pretrained(output_adapter_dir)\n",
        "print(\"Tokenizer saved with adapters.\")\n"
      ],
      "metadata": {
        "id": "Fv1534erZzqO",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f63ebcb1-6bd9-4900-a2a9-c8d3c008680a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Saving LoRA adapters to kiit-llama2-7b-chat-lora-adapters...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/peft/utils/save_and_load.py:230: UserWarning: Setting `save_embedding_layers` to `True` as embedding layers found in `target_modules`.\n",
            "  warnings.warn(\"Setting `save_embedding_layers` to `True` as embedding layers found in `target_modules`.\")\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Adapters saved.\n",
            "Saving tokenizer to kiit-llama2-7b-chat-lora-adapters...\n",
            "Tokenizer saved with adapters.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Step 8: Merge Adapters and Save Final Model Locally\n",
        "\n",
        "This step combines the original Llama 2 weights with your trained LoRA adapters to create a new, standalone model directory. This is essential for easy loading in Streamlit or other applications without needing the PEFT library during inference.\n",
        "\n",
        "**Requires significant RAM/GPU memory.**\n"
      ],
      "metadata": {
        "id": "51itB2uAZ4pt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from peft import AutoPeftModelForCausalLM\n",
        "import gc # Garbage collector\n",
        "\n",
        "# --- Clear Memory Before Loading Full Model ---\n",
        "print(\"Clearing trainer and model from memory...\")\n",
        "del trainer # Delete the trainer object\n",
        "del model   # Delete the LoRA-wrapped model object\n",
        "gc.collect() # Force garbage collection\n",
        "torch.cuda.empty_cache() # Clear GPU cache\n",
        "print(\"Memory cleared.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "W5SJzKUo0-pz",
        "outputId": "184b9562-3a65-46af-cdd5-845fad932349"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Clearing trainer and model from memory...\n",
            "Memory cleared.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# --- Load Base Model (non-quantized or FP16) ---\n",
        "print(f\"Reloading base model ({base_model_name}) for merging...\")\n",
        "# Load in float16 for merging efficiency\n",
        "base_model_reload = AutoModelForCausalLM.from_pretrained(\n",
        "    base_model_name,\n",
        "    return_dict=True,\n",
        "    torch_dtype=torch.float16, # Load in half-precision\n",
        "    device_map='auto', # Load across available GPUs if needed\n",
        "    trust_remote_code=True\n",
        ")\n",
        "print(\"Base model reloaded.\")\n",
        "\n",
        "# --- Load PEFT Model (Adapters) ---\n",
        "print(f\"Loading PEFT adapters from {output_adapter_dir}...\")\n",
        "# Load the PeftModel using the base model and the adapter directory\n",
        "# device_map='auto' will try to load adapters onto the same devices\n",
        "lora_model = PeftModel.from_pretrained(\n",
        "    base_model_reload,\n",
        "    output_adapter_dir,\n",
        "    device_map='auto',\n",
        "    offload_folder=\"offload\" # Create a directory named \"offload\" for this\n",
        ")\n",
        "print(\"PEFT model (adapters) loaded.\")\n",
        "\n",
        "# --- Merge Adapters ---\n",
        "print(\"Merging LoRA adapters into the base model...\")\n",
        "merged_model = lora_model.merge_and_unload()\n",
        "print(\"Adapters merged successfully.\")\n",
        "\n",
        "# --- Save Merged Model ---\n",
        "print(f\"Saving the final merged model to {final_merged_model_dir}...\")\n",
        "# Use safe_serialization=True for better compatibility and safety\n",
        "merged_model.save_pretrained(final_merged_model_dir, safe_serialization=True)\n",
        "print(\"Merged model saved.\")\n",
        "\n",
        "# --- Save Tokenizer with Merged Model ---\n",
        "print(f\"Saving tokenizer to {final_merged_model_dir}...\")\n",
        "# Reload tokenizer associated with the adapters (or use the original one)\n",
        "tokenizer_for_merged = AutoTokenizer.from_pretrained(output_adapter_dir)\n",
        "tokenizer_for_merged.save_pretrained(final_merged_model_dir)\n",
        "print(\"Tokenizer saved with merged model.\")\n",
        "\n",
        "# --- Final Memory Cleanup ---\n",
        "print(\"Cleaning up merged model objects...\")\n",
        "del base_model_reload\n",
        "del lora_model\n",
        "del merged_model\n",
        "gc.collect()\n",
        "torch.cuda.empty_cache()\n",
        "print(\"Cleanup complete.\")\n"
      ],
      "metadata": {
        "id": "poxUHWbTZ6hO",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 295,
          "referenced_widgets": [
            "4c77202c461e48f4b2bd62d500ec4cad",
            "e23253352a134a2cbc47e076eac02bf8",
            "d7eea3bcd2a84fc2a8f7e5d9a36c219a",
            "3ba2099a6b5a4b92b8d3e7e45a859702",
            "469577344502417daeb291a9cea25851",
            "aa8c885dbd004ac299db0034dcb56793",
            "b2d941aa8b264bf2a0051f7387365e20",
            "0dcb612a09864aabba31732fab6a0b5d",
            "b4d82562c0cd4e8bb9148c67155172a0",
            "e50b508edc7842f7b2a670f96306c1cc",
            "7dfe97d0b90a4381b36a21dfdc91291d"
          ]
        },
        "outputId": "07fd624e-d74f-4c02-f4fc-edec098e3d01"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Reloading base model (meta-llama/Llama-2-7b-chat-hf) for merging...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/huggingface_hub/utils/_auth.py:94: UserWarning: \n",
            "The secret `HF_TOKEN` does not exist in your Colab secrets.\n",
            "To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n",
            "You will be able to reuse this secret in all of your notebooks.\n",
            "Please note that authentication is recommended but still optional to access public models or datasets.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "4c77202c461e48f4b2bd62d500ec4cad"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:accelerate.big_modeling:Some parameters are on the meta device because they were offloaded to the cpu.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Base model reloaded.\n",
            "Loading PEFT adapters from kiit-llama2-7b-chat-lora-adapters...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:accelerate.big_modeling:Some parameters are on the meta device because they were offloaded to the disk and cpu.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "PEFT model (adapters) loaded.\n",
            "Merging LoRA adapters into the base model...\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# merge_script.py\n",
        "import torch\n",
        "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
        "from peft import PeftModel\n",
        "import gc\n",
        "import os\n",
        "\n",
        "base_model_name = \"meta-llama/Llama-2-7b-chat-hf\"\n",
        "adapter_dir = \"kiit-llama2-7b-chat-lora-adapters\" # Path to downloaded adapters\n",
        "output_merged_dir = \"kiit-llama2-7b-chat-final\" # Output path\n",
        "\n",
        "print(f\"Loading base model: {base_model_name}\")\n",
        "# Load on CPU first if RAM is plentiful, or use device_map if GPU available\n",
        "base_model = AutoModelForCausalLM.from_pretrained(\n",
        "    base_model_name,\n",
        "    return_dict=True,\n",
        "    torch_dtype=torch.float16, # Or desired final precision\n",
        "    # device_map=\"auto\", # Optional if GPU available in merge environment\n",
        "    low_cpu_mem_usage=True # Helps on systems with high RAM but slower disk\n",
        ")\n",
        "print(\"Base model loaded.\")\n",
        "\n",
        "print(f\"Loading adapters from: {adapter_dir}\")\n",
        "# Load adapters onto the base model\n",
        "model_to_merge = PeftModel.from_pretrained(\n",
        "    base_model,\n",
        "    adapter_dir\n",
        "    # device_map=\"auto\" # Optional\n",
        ")\n",
        "print(\"Adapters loaded.\")\n",
        "\n",
        "print(\"Merging adapters...\")\n",
        "merged_model = model_to_merge.merge_and_unload()\n",
        "print(\"Merge complete.\")\n",
        "\n",
        "print(f\"Saving merged model to: {output_merged_dir}\")\n",
        "merged_model.save_pretrained(output_merged_dir, safe_serialization=True)\n",
        "print(\"Merged model saved.\")\n",
        "\n",
        "print(f\"Saving tokenizer to: {output_merged_dir}\")\n",
        "tokenizer = AutoTokenizer.from_pretrained(adapter_dir) # Load tokenizer from adapter dir\n",
        "tokenizer.save_pretrained(output_merged_dir)\n",
        "print(\"Tokenizer saved.\")\n",
        "\n",
        "print(\"Merge script finished.\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 136,
          "referenced_widgets": [
            "f7807694a26642aaa0823cbe756b35ee",
            "52f96d8dd15b48deabd7c220fe7a56ce",
            "c2b2194ce98a4ec6b4f147521c5c0d37",
            "973c643070c54575a3cbb0bddd194b42",
            "23740995c9594ec794c91ea26bc4b282",
            "31e5af81d57741d38ee568449fbcd4ee",
            "e1542165bc7a43b8ab2df35402850244",
            "fd7d60962d0f46b5999bf84bccc27712",
            "4e7a61aa2cbc4d24861cf9689b538f2c",
            "a8f253f1570448009b1b90b70486a427",
            "917a9a1c3de54a7ab2ba7659be6e2dc4"
          ]
        },
        "id": "2hHfPb4q9hSd",
        "outputId": "b17bf970-7008-4070-9153-5f932c7c769a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loading base model: meta-llama/Llama-2-7b-chat-hf\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "f7807694a26642aaa0823cbe756b35ee"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Base model loaded.\n",
            "Loading adapters from: kiit-llama2-7b-chat-lora-adapters\n",
            "Adapters loaded.\n",
            "Merging adapters...\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Step 9: Test Inference with Fine-tuned Merged Model\n",
        "\n",
        "Load the final standalone model saved locally and test its QnA capabilities on KIIT-specific questions.\n"
      ],
      "metadata": {
        "id": "xUC0kU82Z9R6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from transformers import AutoModelForCausalLM, AutoTokenizer, pipeline\n",
        "import gc\n",
        "\n",
        "# --- Load the Final Merged Model ---\n",
        "print(f\"Loading the final fine-tuned model from: {final_merged_model_dir}\")\n",
        "\n",
        "# Ensure torch_dtype matches how you saved the merged model (float16 in this case)\n",
        "ft_model = AutoModelForCausalLM.from_pretrained(\n",
        "    final_merged_model_dir,\n",
        "    torch_dtype=torch.float16,\n",
        "    device_map=\"auto\", # Load onto available GPU(s)\n",
        ")\n",
        "ft_tokenizer = AutoTokenizer.from_pretrained(final_merged_model_dir)\n",
        "\n",
        "print(\"Final fine-tuned model and tokenizer loaded.\")\n",
        "\n",
        "# --- Set up Generation Pipeline ---\n",
        "# device_map=\"auto\" should handle device placement\n",
        "pipe = pipeline(\n",
        "    \"text-generation\",\n",
        "    model=ft_model,\n",
        "    tokenizer=ft_tokenizer,\n",
        "    torch_dtype=torch.float16,\n",
        "    device_map=\"auto\" # Redundant but safe\n",
        ")\n",
        "\n",
        "# --- Test Prompts ---\n",
        "def ask_kiit_bot(question):\n",
        "    \"\"\"Generates an answer using the fine-tuned model and Llama 2 chat format.\"\"\"\n",
        "    system_prompt = \"You are a helpful assistant knowledgeable about Kalinga Institute of Industrial Technology (KIIT). Provide concise and accurate information based on the user's question about KIIT.\"\n",
        "    prompt = f\"<s>[INST] <<SYS>>\\n{system_prompt}\\n<</SYS>>\\n\\n{question} [/INST]\"\n",
        "\n",
        "    print(f\"\\n--- Testing Prompt ---\\n{question}\\n\")\n",
        "    sequences = pipe(\n",
        "        prompt,\n",
        "        do_sample=True,       # Enable sampling for more varied responses\n",
        "        top_k=10,             # Consider top 10 probable tokens\n",
        "        num_return_sequences=1,\n",
        "        eos_token_id=ft_tokenizer.eos_token_id,\n",
        "        max_new_tokens=200    # Limit the answer length\n",
        "    )\n",
        "\n",
        "    print(\"--- Generated Response ---\")\n",
        "    full_response = sequences[0]['generated_text']\n",
        "    # Extract only the text after [/INST]\n",
        "    answer_part = full_response.split('[/INST]')[-1].strip()\n",
        "    # Remove potential EOS token if it appears right at the end\n",
        "    if answer_part.endswith(ft_tokenizer.eos_token):\n",
        "         answer_part = answer_part[:-len(ft_tokenizer.eos_token)].strip()\n",
        "    print(answer_part)\n",
        "    print(\"-\" * 26)\n",
        "\n",
        "\n",
        "# Test Case 1\n",
        "ask_kiit_bot(\"What is the fee structure for B.Tech CSE?\")\n",
        "\n",
        "# Test Case 2\n",
        "ask_kiit_bot(\"How many schools does KIIT have?\")\n",
        "\n",
        "# Test Case 3\n",
        "ask_kiit_bot(\"Tell me about the placement process at KIIT.\")\n",
        "\n",
        "# Test Case 4 (More conversational)\n",
        "ask_kiit_bot(\"Is the campus life good at KIIT?\")\n",
        "\n",
        "# --- Cleanup ---\n",
        "# del ft_model\n",
        "# del ft_tokenizer\n",
        "# del pipe\n",
        "# gc.collect()\n",
        "# torch.cuda.empty_cache()\n"
      ],
      "metadata": {
        "id": "_rmdIhWhaECr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        " ## Step 10: Test Inference using Adapters Directly (No Merge)"
      ],
      "metadata": {
        "id": "yK46qnaHBGze"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# --- Step 10: Test Inference using Adapters Directly (No Merge) ---\n",
        "\n",
        "import torch\n",
        "from transformers import AutoModelForCausalLM, AutoTokenizer, pipeline, BitsAndBytesConfig\n",
        "from peft import PeftModel # Import PeftModel\n",
        "import gc\n",
        "import os\n",
        "\n",
        "# --- Configuration ---\n",
        "# Ensure these are defined from previous steps\n",
        "base_model_name = \"meta-llama/Llama-2-7b-chat-hf\"\n",
        "adapter_dir = \"kiit-llama2-7b-chat-lora-adapters\" # Directory where adapters were saved\n",
        "\n",
        "# --- Define Quantization Config (Needed again for loading base model) ---\n",
        "# Determine compute dtype based on GPU capability check\n",
        "major, minor = torch.cuda.get_device_capability() if torch.cuda.is_available() else (0, 0)\n",
        "if major >= 8: # Ampere or newer\n",
        "    compute_dtype = torch.bfloat16\n",
        "    print(\"Using bfloat16.\")\n",
        "else:\n",
        "    compute_dtype = torch.float16\n",
        "    print(\"Using float16.\")\n",
        "\n",
        "bnb_config_inference = BitsAndBytesConfig(\n",
        "    load_in_4bit=True,\n",
        "    bnb_4bit_quant_type=\"nf4\",\n",
        "    bnb_4bit_compute_dtype=compute_dtype,\n",
        "    bnb_4bit_use_double_quant=True, # Match training config if possible\n",
        ")\n",
        "\n",
        "# --- Load Quantized Base Model ---\n",
        "print(f\"Loading base model ({base_model_name}) quantized...\")\n",
        "base_model_inference = AutoModelForCausalLM.from_pretrained(\n",
        "    base_model_name,\n",
        "    quantization_config=bnb_config_inference,\n",
        "    device_map=\"auto\",\n",
        "    trust_remote_code=True\n",
        "    # token=\"YOUR_HF_TOKEN\" # Add if needed\n",
        ")\n",
        "print(\"Base model loaded.\")\n",
        "\n",
        "# --- Load Tokenizer ---\n",
        "# Load tokenizer from the adapter directory (where it was saved)\n",
        "print(f\"Loading tokenizer from {adapter_dir}...\")\n",
        "tokenizer_inference = AutoTokenizer.from_pretrained(adapter_dir)\n",
        "print(\"Tokenizer loaded.\")\n",
        "\n",
        "# --- Load LoRA Adapters onto the Base Model ---\n",
        "print(f\"Loading LoRA adapters from {adapter_dir} onto the base model...\")\n",
        "# This automatically uses the device map from the base model\n",
        "model_with_adapters = PeftModel.from_pretrained(base_model_inference, adapter_dir)\n",
        "print(\"LoRA adapters loaded.\")\n",
        "\n",
        "# --- Set up Pipeline ---\n",
        "print(\"Setting up text generation pipeline...\")\n",
        "# Use the model with adapters loaded\n",
        "pipe = pipeline(\n",
        "    \"text-generation\",\n",
        "    model=model_with_adapters, # Use the PEFT model\n",
        "    tokenizer=tokenizer_inference,\n",
        "    torch_dtype=compute_dtype, # Match the compute dtype\n",
        "    device_map=\"auto\"\n",
        ")\n",
        "print(\"Pipeline ready.\")\n",
        "\n",
        "# --- Test Inference ---\n",
        "def ask_kiit_bot_adapters(question):\n",
        "    \"\"\"Generates response using the model with adapters.\"\"\"\n",
        "    system_prompt = \"You are a helpful assistant knowledgeable about Kalinga Institute of Industrial Technology (KIIT). Provide concise and accurate information based on the user's question about KIIT.\"\n",
        "    prompt = f\"<s>[INST] <<SYS>>\\n{system_prompt}\\n<</SYS>>\\n\\n{question} [/INST]\"\n",
        "\n",
        "    print(f\"\\n--- Testing Prompt ---\\n{question}\\n\")\n",
        "    sequences = pipe(\n",
        "        prompt,\n",
        "        do_sample=True,\n",
        "        top_k=10,\n",
        "        num_return_sequences=1,\n",
        "        eos_token_id=tokenizer_inference.eos_token_id,\n",
        "        max_new_tokens=250\n",
        "    )\n",
        "\n",
        "    print(\"--- Generated Response ---\")\n",
        "    full_response = sequences[0]['generated_text']\n",
        "    answer_part = full_response.split('[/INST]')[-1].strip()\n",
        "    if answer_part.endswith(tokenizer_inference.eos_token):\n",
        "         answer_part = answer_part[:-len(tokenizer_inference.eos_token)].strip()\n",
        "    print(answer_part)\n",
        "    print(\"-\" * 26)\n",
        "\n",
        "# --- Run Tests ---\n",
        "ask_kiit_bot_adapters(\"What is the eligibility criteria for B.Tech programs at KIIT?\")\n",
        "ask_kiit_bot_adapters(\"Describe the campus infrastructure.\")\n",
        "ask_kiit_bot_adapters(\"Who is the founder of KIIT?\")\n",
        "\n",
        "# --- Cleanup ---\n",
        "del base_model_inference\n",
        "del model_with_adapters\n",
        "del tokenizer_inference\n",
        "del pipe\n",
        "gc.collect()\n",
        "torch.cuda.empty_cache()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 624,
          "referenced_widgets": [
            "446fd3c6f9ae4983b28b66989d79e242",
            "86c2f6882648449ab0f88af038c86c9a",
            "03f2a98df14e4e679a159dcf09f8bb42",
            "a9fedf156bc74dfab74ac7095f24d9e5",
            "4929b1fe96044a778414308792f3768d",
            "ee386f0a6fc84349b7006d2c348ca629",
            "8ff4929ced5c49608a956e5de3055f92",
            "578363f9bb2f4fdb8f61c0459b26efe6",
            "bab683bfb2b845d488a95b15008bfdef",
            "99bb320e1791452abab596580854183a",
            "b921b00859cf4a149b7834bd81bfc9f9"
          ]
        },
        "id": "-u7eQYA5BOl9",
        "outputId": "7a0b5c8f-9347-43c1-d904-f4ed4cbbfbb7"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Using float16.\n",
            "Loading base model (meta-llama/Llama-2-7b-chat-hf) quantized...\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "446fd3c6f9ae4983b28b66989d79e242"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Base model loaded.\n",
            "Loading tokenizer from kiit-llama2-7b-chat-lora-adapters...\n",
            "Tokenizer loaded.\n",
            "Loading LoRA adapters from kiit-llama2-7b-chat-lora-adapters onto the base model...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Device set to use cuda:0\n",
            "The model 'PeftModelForCausalLM' is not supported for text-generation. Supported models are ['AriaTextForCausalLM', 'BambaForCausalLM', 'BartForCausalLM', 'BertLMHeadModel', 'BertGenerationDecoder', 'BigBirdForCausalLM', 'BigBirdPegasusForCausalLM', 'BioGptForCausalLM', 'BlenderbotForCausalLM', 'BlenderbotSmallForCausalLM', 'BloomForCausalLM', 'CamembertForCausalLM', 'LlamaForCausalLM', 'CodeGenForCausalLM', 'CohereForCausalLM', 'Cohere2ForCausalLM', 'CpmAntForCausalLM', 'CTRLLMHeadModel', 'Data2VecTextForCausalLM', 'DbrxForCausalLM', 'DeepseekV3ForCausalLM', 'DiffLlamaForCausalLM', 'ElectraForCausalLM', 'Emu3ForCausalLM', 'ErnieForCausalLM', 'FalconForCausalLM', 'FalconMambaForCausalLM', 'FuyuForCausalLM', 'GemmaForCausalLM', 'Gemma2ForCausalLM', 'Gemma3ForConditionalGeneration', 'Gemma3ForCausalLM', 'GitForCausalLM', 'GlmForCausalLM', 'GotOcr2ForConditionalGeneration', 'GPT2LMHeadModel', 'GPT2LMHeadModel', 'GPTBigCodeForCausalLM', 'GPTNeoForCausalLM', 'GPTNeoXForCausalLM', 'GPTNeoXJapaneseForCausalLM', 'GPTJForCausalLM', 'GraniteForCausalLM', 'GraniteMoeForCausalLM', 'GraniteMoeSharedForCausalLM', 'HeliumForCausalLM', 'JambaForCausalLM', 'JetMoeForCausalLM', 'LlamaForCausalLM', 'Llama4ForCausalLM', 'Llama4ForCausalLM', 'MambaForCausalLM', 'Mamba2ForCausalLM', 'MarianForCausalLM', 'MBartForCausalLM', 'MegaForCausalLM', 'MegatronBertForCausalLM', 'MistralForCausalLM', 'MixtralForCausalLM', 'MllamaForCausalLM', 'MoshiForCausalLM', 'MptForCausalLM', 'MusicgenForCausalLM', 'MusicgenMelodyForCausalLM', 'MvpForCausalLM', 'NemotronForCausalLM', 'OlmoForCausalLM', 'Olmo2ForCausalLM', 'OlmoeForCausalLM', 'OpenLlamaForCausalLM', 'OpenAIGPTLMHeadModel', 'OPTForCausalLM', 'PegasusForCausalLM', 'PersimmonForCausalLM', 'PhiForCausalLM', 'Phi3ForCausalLM', 'Phi4MultimodalForCausalLM', 'PhimoeForCausalLM', 'PLBartForCausalLM', 'ProphetNetForCausalLM', 'QDQBertLMHeadModel', 'Qwen2ForCausalLM', 'Qwen2MoeForCausalLM', 'Qwen3ForCausalLM', 'Qwen3MoeForCausalLM', 'RecurrentGemmaForCausalLM', 'ReformerModelWithLMHead', 'RemBertForCausalLM', 'RobertaForCausalLM', 'RobertaPreLayerNormForCausalLM', 'RoCBertForCausalLM', 'RoFormerForCausalLM', 'RwkvForCausalLM', 'Speech2Text2ForCausalLM', 'StableLmForCausalLM', 'Starcoder2ForCausalLM', 'TransfoXLLMHeadModel', 'TrOCRForCausalLM', 'WhisperForCausalLM', 'XGLMForCausalLM', 'XLMWithLMHeadModel', 'XLMProphetNetForCausalLM', 'XLMRobertaForCausalLM', 'XLMRobertaXLForCausalLM', 'XLNetLMHeadModel', 'XmodForCausalLM', 'ZambaForCausalLM', 'Zamba2ForCausalLM'].\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "LoRA adapters loaded.\n",
            "Setting up text generation pipeline...\n",
            "Pipeline ready.\n",
            "\n",
            "--- Testing Prompt ---\n",
            "What is the eligibility criteria for B.Tech programs at KIIT?\n",
            "\n",
            "--- Generated Response ---\n",
            "12th with 60%+ in PCM subjects. 100% seats reserved for Odisha applicants. 2024: 1.6 Lacs applicants.\n",
            "--------------------------\n",
            "\n",
            "--- Testing Prompt ---\n",
            "Describe the campus infrastructure.\n",
            "\n",
            "--- Generated Response ---\n",
            "150+ buildings with 100% Wi-Fi. 120+ sports facilities. 550+ CCTV cameras. 24/7 security guards.\n",
            "--------------------------\n",
            "\n",
            "--- Testing Prompt ---\n",
            "Who is the founder of KIIT?\n",
            "\n",
            "--- Generated Response ---\n",
            "Dr. Achyuta Samanta. Founded KIIT in 1992 as a tribal university. 2008 UGC Act gave it central status.\n",
            "--------------------------\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#DOWNLOAD Adapters folder\n",
        "!zip -r /content/kiit-llama2-7b-chat-lora-adapters.zip /content/kiit-llama2-7b-chat-lora-adapters\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "r9zKfasLDPYX",
        "outputId": "76397a57-6b7f-4c62-ccca-aefa8c7a5f82"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  adding: content/kiit-llama2-7b-chat-lora-adapters/ (stored 0%)\n",
            "  adding: content/kiit-llama2-7b-chat-lora-adapters/README.md (deflated 66%)\n",
            "  adding: content/kiit-llama2-7b-chat-lora-adapters/tokenizer_config.json (deflated 66%)\n",
            "  adding: content/kiit-llama2-7b-chat-lora-adapters/special_tokens_map.json (deflated 73%)\n",
            "  adding: content/kiit-llama2-7b-chat-lora-adapters/tokenizer.model (deflated 55%)\n",
            "  adding: content/kiit-llama2-7b-chat-lora-adapters/adapter_config.json (deflated 56%)\n",
            "  adding: content/kiit-llama2-7b-chat-lora-adapters/tokenizer.json (deflated 85%)\n",
            "  adding: content/kiit-llama2-7b-chat-lora-adapters/adapter_model.safetensors (deflated 28%)\n"
          ]
        }
      ]
    }
  ]
}